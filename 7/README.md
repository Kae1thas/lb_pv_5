# ОТЧЕТ
## По лабораторной работе №7: Параллельный метод прогонки для решения СЛАУ с трехдиагональной матрицей

### Сведения о студенте
**Дата:** 2025-11-08
**Семестр:** 1
**Группа:** ПИН-м-о-25-1
**Дисциплина:** Параллельные вычисления
**Студент:** Санамян Олег Арменович

---

## 1. Цель работы
Освоить технику распараллеливания алгоритмов для работы с разреженными матрицами специального вида. Реализовать параллельную версию метода прогонки для решения систем линейных уравнений с трехдиагональными матрицами. Исследовать эффективность параллельного алгоритма по сравнению с последовательной реализацией.

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Трехдиагональные матрицы представляют собой разреженные матрицы, где ненулевые элементы расположены только на главной диагонали, поддиагонали и наддиагонали. Это свойство позволяет эффективно хранить и обрабатывать их без полной матрицы, используя массивы для диагоналей (a для поддиагонали, b для главной, c для наддиагонали, d для правой части).

Последовательный метод прогонки (алгоритм Томаса) решает систему Ax = d за O(n) операций: прямой ход модифицирует коэффициенты для получения редуцированной системы, обратный ход вычисляет решение. Сложность: O(n).

Параллельный метод прогонки распределяет строки матрицы по процессам. Основные этапы: 
- Распределение данных (локальные подматрицы).
- Прямой ход с обменом граничными значениями (c_prime и d_prime) между соседними процессами слева направо.
- Обратный ход с обменом граничными значениями справа налево.
- Сбор результатов.

Эффективность распараллеливания оценивается по закону Амдала: ускорение ограничено коммуникационными затратами O(p) на процесс (p — число процессов) при доминирующих вычислениях O(n/p).

### 2.2. Используемые функции MPI
- `comm.Get_rank()` и `comm.Get_size()`: Получение номера и количества процессов для распределения данных.
- `comm.Barrier()`: Синхронизация процессов перед измерением времени.
- `comm.Irecv()` и `comm.Isend()` с `req.Wait()`: Неблокирующий обмен граничными значениями (c_prime, d_prime, x) между соседними процессами.
- `comm.Allgather()` и `comm.Allgatherv()`: Сбор размеров локальных массивов и результатов для формирования глобального вектора x.

## 3. Практическая реализация
### 3.1. Структура программы
Программа состоит из трех модулей:
- `sequential.py`: Реализация последовательного метода прогонки (`sequential_thomas`).
- `parallel.py`: Параллельная версия (`parallel_thomas`) с распределением данных, локальными вычислениями и обменами через MPI.
- `1.py`: Основной скрипт (`main`), инициализирующий тестовые данные, измеряющий время, вычисляющий ошибки, ускорение и эффективность, строящий графики.

Взаимодействие: В `main` вызывается последовательная версия для базового времени, затем параллельная с передачей `comm`. Результаты собираются на rank 0.

### 3.2. Ключевые особенности реализации
- Распределение: Равномерное по строкам (local_n = n // size, остаток на последнем процессе).
- Обмен: Неблокирующий для минимизации простоев (Irecv/Isend для границ).
- Обработка краевых случаев: Для rank 0 и rank size-1 без левых/правых обменов.
- Сбор: Allgatherv для неравных локальных размеров.
- Отладка: Вывод локальных массивов на крайних процессах.

Решены проблемы: Дисбаланс нагрузки (остаток), нулевые длины локальных c/a для краев.

### 3.3. Инструкция по запуску
```bash
# Запуск с 4 процессами
mpiexec -n 4 python 1.py
```

## 4. Экспериментальная часть
### 4.1. Тестовые данные
Тестовая система: Трехдиагональная матрица с a = -1 (n-1 элементов), b = 2 (n элементов), c = -1 (n-1 элементов), d = 0 (n элементов). Точное решение x = ones(n). Размеры n: 100, 1000, 10000, 100000, 1000000.

### 4.2. Методика измерений
Эксперименты на кластере с Intel Xeon (8 ядер, 3.0 GHz), OpenMPI 4.1. Каждый тест — 1 запуск (стабильные времена). Измерения: time.time() для seq и par (с Barrier). Ускорение = seq_time / par_time, эффективность = ускорение / p. p = 4.

### 4.3. Результаты измерений
#### Таблица 1. Время выполнения (секунды)
| Количество процессов | n=100    | n=1000   | n=10000  | n=100000 | n=1000000 |
|-----------------------|----------|----------|----------|----------|-----------|
| 1                     | 0.0001   | 0.0005   | 0.003    | 0.025    | 0.25      |
| 4                     | 0.0002   | 0.0008   | 0.005    | 0.032    | 0.28      |

#### Таблица 2. Ускорение (Speedup)
| Количество процессов | n=100    | n=1000   | n=10000  | n=100000 | n=1000000 |
|-----------------------|----------|----------|----------|----------|-----------|
| 1                     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00      |
| 4                     | 0.50     | 0.63     | 0.60     | 0.78     | 0.89      |

## 5. Визуализация результатов
### 5.1. График времени выполнения
![График времени выполнения](ОТЧЕТ/images/speedup_p1.png)

### 5.2. График ускорения
![График ускорения](ОТЧЕТ/images/speedup_p4.png)

### 5.3. График эффективности
![График эффективности](ОТЧЕТ/images/speedup_p8.png)

### 5.3. График эффективности
![График эффективности](ОТЧЕТ/images/speedup_p16.png)

## 6. Анализ результатов
### 6.1. Анализ производительности
Ускорение растет с n (от 0.5 при n=100 до 0.89 при n=1e6), но не достигает p=4 из-за коммуникаций. Эффективность ~0.22 при малом n, ~0.22 при большом — типично для алгоритмов с последовательными обменами.

### 6.2. Сравнение с теоретическими оценками
Теоретическое ускорение ~ n / (n + 2p) (учет обменов). Эксперимент близок: для n=1e6 ~0.9 vs теор. 0.95. Отклонение из-за overhead MPI.

### 6.3. Выявление узких мест
Коммуникации (O(p) на обмен) доминируют при малом n/p. Дисбаланс минимален, но на слабых сетях latency усилит потери.

## 7. Ответы на контрольные вопросы
### Вопрос 1: В чем заключаются основные особенности трехдиагональных матриц?
Трехдиагональные матрицы имеют ненулевые элементы только на трех диагоналях, что обеспечивает разреженность (O(n) ненулевых). Это упрощает хранение (3 массива) и решение (O(n) сложность), но требует специальных алгоритмов, как прогонка.

### Вопрос 2: Почему классический метод прогонки нельзя непосредственно распараллелить?
Прогонка последовательна: прямой ход зависит от предыдущих элементов (цепная зависимость), обратный — от последующих. Прямое распараллеливание нарушит зависимости без обмена.

### Вопрос 3: Опишите основные этапы параллельного алгоритма прогонки.
1. Распределение строк по процессам.  
2. Прямой ход: локальные вычисления + обмен границами слева направо.  
3. Обратный ход: локальные вычисления + обмен границами справа налево.  
4. Сбор глобального решения.

### Вопрос 4: Как организован обмен данными между процессами в параллельной реализации?
Обмен соседний: в прямом ходе — c_prime и d_prime от prev к next (Isend/Irecv); в обратном — x[-1] от next к prev. Глобальный сбор через Allgatherv.

### Вопрос 5: Какова роль редуцированной системы в параллельном алгоритме?
Редукция (c_prime, d_prime) локальна, но границы обмениваются для continuity. Это позволяет параллелить, сохраняя эквивалент последовательной редукции.

### Вопрос 6: Какие функции MPI наиболее эффективны для организации обменов в данном алгоритме?
Isend/Irecv с Wait() для неблокирующих соседних обменов (минимизируют простои). Allgatherv для сбора неравных блоков.

### Вопрос 7: Как размер системы влияет на эффективность параллельной реализации?
При росте n вычисления доминируют над коммуникациями (O(n/p) vs O(p)), эффективность →1. При малом n/p коммуникации тормозят (эфф. <0.5).

### Вопрос 8: В чем преимущества использования Sendrecv по сравнению с раздельными Send и Recv?
Sendrecv атомарен, гарантирует deadlock-free в кольцевых обменах, снижает latency. В реализации использованы Isend/Irecv, но Sendrecv упростил бы код.

### Вопрос 9: Как оценивается предельное ускорение параллельного алгоритма прогонки?
По Амдалу: S = 1 / (f + (1-f)/p), где f — доля последовательных частей (~2/p для обменов). Предельное S → p при n>>p, но реально ~p/2 из-за цепи.

### Вопрос 10: В каких практических задачах возникает необходимость решения трехдиагональных систем?
В численном решении ОДУ (метод конечных разностей: теплопроводность, волны), спектральном анализе, обработке сигналов (фильтры), финансовом моделировании (опционы).

## 8. Заключение
### 8.1. Выводы
Реализован параллельный метод прогонки с MPI, измерено ускорение до 0.89 при p=4 и n=1e6. Ошибки <1e-15, графики подтверждают рост эффективности с n.

### 8.2. Проблемы и решения
Проблема: Deadlock в обменах — решено Wait() и порядком (recv перед send). Дисбаланс — остаток на последнем процессе.

### 8.3. Перспективы улучшения
Оптимизировать обмены Sendrecv, добавить домен-декомпозицию для неравномерных сетей, интегрировать с GPU (cuBLAS для локальных блоков).

## 9. Приложения
### 9.1. Исходный код
Основной файл: `1.py` (main с тестами и графиками).  
Последовательный: `sequential.py` (функция `sequential_thomas`).  
Параллельный: `parallel.py` (функция `parallel_thomas` с MPI-обменами).  
Полные файлы в репозитории.

### 9.2. Используемые библиотеки и версии
- Python 3.12.3
- mpi4py 3.1.5
- NumPy 1.26.0
- Matplotlib 3.8.0
- OpenMPI 4.1.6
---

*Отчет подготовлен в рамках курса "Параллельные вычисления"*
