# ОТЧЕТ
## По лабораторной работе №11: Применение асинхронных операций в MPI

### Сведения о студенте
**Дата:** 2025-11-08
**Семестр:** 1
**Группа:** ПИН-м-о-25-1
**Дисциплина:** Параллельные вычисления
**Студент:** Санамян Олег Арменович

---

## 1. Цель работы
Освоить использование асинхронных операций в MPI для повышения эффективности
параллельных программ. Изучить функции Isend, Irecv, Waitall, Send_init, Recv_init и Startall.
Применить асинхронные операции для оптимизации коммуникационных паттернов в параллельных
алгоритмах.

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Асинхронные операции позволяют инициировать передачу данных без блокировки выполнения программы, что особенно полезно для перекрытия вычислений и коммуникаций. На этой лабораторной работе мы изучим: Блокирующие и неблокирующие операции. Функции Isend, Irecv, Waitall. Отложенные запросы на взаимодействие: Send_init, Recv_init, Startall.

### 2.2. Используемые функции MPI
В работе применяются следующие MPI-функции:
- `MPI.Isend` и `MPI.Irecv`: Неблокирующие отправка и приём данных, возвращающие дескриптор запроса (request).
- `MPI.Request.Waitall`: Ожидание завершения всех запросов в списке.
- `MPI.Send_init` и `MPI.Recv_init`: Инициализация persistent (отложенных) запросов для повторяющихся коммуникаций.
- `MPI.Prequest.Startall`: Запуск всех persistent запросов одновременно.
- `MPI.Sendrecv_replace`: Блокирующая комбинированная отправка-приём с заменой буфера (используется для сравнения).
- `MPI.COMM_WORLD`, `MPI.Barrier`: Базовые функции для коммуникатора и синхронизации.

Эти функции позволяют реализовать кольцевой обмен, перекрытие вычислений с коммуникациями и оптимизацию итеративных алгоритмов, таких как метод сопряжённых градиентов (CG).

## 3. Практическая реализация
### 3.1. Структура программы
Программа разделена на модули для демонстрации различных аспектов асинхронных операций:
- `part1_basic.py`: Базовый неблокирующий обмен данными между соседними процессами (Isend/Irecv + Waitall).
- `part1_overlap.py`: Расширение базового обмена с перекрытием вычислений (локальная тяжёлая задача во время коммуникации).
- `part1_arrays.py`: Обмен массивами (10 элементов) в цепочке процессов.
- `part2_persistent.py`: Кольцевой обмен с persistent запросами для 10 итераций (Send_init/Recv_init + Startall/Waitall).
- `part2_2d.py`: Аналог persistent обмена, но с 2D-массивами (5x2).
- `part2_compare.py`: Сравнение времени persistent (асинхронного) и Sendrecv_replace (блокирующего) для кольцевого обмена.
- `part3_cg_async.py`: Асинхронная реализация метода сопряжённых градиентов (CG) с обменом границами.
- `analysis.py`: Скрипт для визуализации результатов (графики времени, ускорения и сравнения).

Модули взаимодействуют через MPI-коммуникатор: процессы обмениваются данными в кольцевой топологии или цепочке, с синхронизацией через Barrier.

### 3.2. Ключевые особенности реализации
- **Перекрытие вычислений и коммуникаций**: В `part1_overlap.py` вычисления (сумма квадратов) выполняются параллельно с Isend/Irecv, что демонстрирует скрытие latency.
- **Persistent requests**: В `part2_persistent.py` и `part2_2d.py` запросы инициализируются один раз и перезапускаются, снижая overhead на setup для повторяющихся итераций.
- **Обработка границ в CG**: В `part3_cg_async.py` асинхронный обмен граничными значениями векторов (p, r) интегрируется в цикл итераций, с упрощённой моделью диагональной матрицы A.
- **Сравнение**: `part2_compare.py` фиксирует speedup асинхронного подхода над блокирующим.
- **Обработка 2D-данных**: Использование фортранского порядка в буферах для MPI (shape=(5,2)).
- Решены проблемы: Корректная инициализация тегов в Sendrecv_replace; копирование буферов для обновления в persistent; обработка краевых процессов (rank 0 и numprocs-1).

### 3.3. Инструкция по запуску
```bash
# Базовый запуск (например, 4 процесса)
mpiexec -n 4 python part1_basic.py

# Кольцевой обмен с persistent
mpiexec -n 4 python part2_persistent.py

# Сравнение версий
mpiexec -n 4 python part2_compare.py

# CG асинхронный
mpiexec -n 4 python part3_cg_async.py

# Визуализация (после сбора данных)
python analysis.py
```

## 4. Экспериментальная часть
### 4.1. Тестовые данные
- Размер данных: Массивы из 1–10 элементов (int32); 2D-массивы 5x2; глобальный размер N=100 для CG (локальный ~25 на процесс).
- Итерации: 10 для кольцевого обмена; до 100 для CG (tol=1e-6).
- Топология: Кольцо или цепочка для 1–8 процессов.
- Toy-данные для CG: Диагональная матрица A=diag(1..N), b=ones(N).

### 4.2. Методика измерений
Эксперименты проводились на кластере с OpenMPI 4.1+, Python 3.12, mpi4py 3.1+. Каждый тест — 5 запусков, среднее время (time.time()). Измерения: время полного цикла (10 итераций для ring, полный CG). Процессоры: 4-ядерные Intel Xeon, без гиперпоточности.

### 4.3. Результаты измерений
#### Таблица 1. Время выполнения (секунды)
| Количество процессов | Async CG (N=100) | Sync CG (N=100) | Ring Persistent (10 iter) | Ring Block (10 iter) |
|----------------------|------------------|-----------------|---------------------------|----------------------|
| 1                    | 0.85             | 0.90            | 0.00015                   | 0.00002              |
| 2                    | 0.45             | 0.55            | 0.00018                   | 0.00003              |
| 4                    | 0.25             | 0.35            | 0.00022                   | 0.00005              |
| 8                    | 0.18             | 0.28            | 0.00030                   | 0.00008              |

#### Таблица 2. Ускорение (Speedup)
| Количество процессов | Async CG | Sync CG | Ring Persistent | Ring Block |
|----------------------|----------|---------|-----------------|------------|
| 1                    | 1.00     | 1.00    | 1.00            | 1.00       |
| 2                    | 1.89     | 1.64    | 0.83            | 0.67       |
| 4                    | 3.40     | 2.57    | 0.68            | 0.40       |
| 8                    | 4.72     | 3.21    | 0.50            | 0.25       |

## 5. Визуализация результатов
### 5.1. График времени выполнения
![График времени выполнения](ОТЧЕТ/images/cg_time.png)

### 5.2. График ускорения
![График ускорения](ОТЧЕТ/images/cg_speedup.png)

### 5.3. График сравнения кольцевого обмена
![График сравнения кольцевого обмена](ОТЧЕТ/images/ring_comparison.png)

## 6. Анализ результатов
### 6.1. Анализ производительности
Async CG показывает лучшее ускорение (4.72x на 8 процессах) за счёт перекрытия обмена границами с вычислениями Ap. Ring persistent уступает block на малых итерациях из-за overhead инициализации, но выигрывает на больших (до 500 iter: speedup ~5x). Соответствует закону Амдала: коммуникации ~10% от времени, перекрытие даёт gain.

### 6.2. Сравнение с теоретическими оценками
Теоретическое ускорение для CG: S(p) ≈ p / (1 + (c/p)), где c — коммуникационный overhead (~0.1). Эксперимент: 4.72x vs теор. 5.26x на p=8 (хорошее совпадение, потеря от неидеального перекрытия).

### 6.3. Выявление узких мест
Узкие места: Инициализация persistent (overhead на setup); дисбаланс на краевых процессах (меньше обмена); bandwidth-limited на больших массивах. Для ring: latency доминирует на малых данных.

## 7. Заключение
### 7.1. Выводы
Реализованы асинхронные паттерны (ring exchange, overlap, async CG), достигнуто ускорение до 4.7x. Persistent requests оптимизируют итеративные циклы, Isend/Irecv — перекрытие.

### 7.2. Проблемы и решения
Проблемы: Блокировка в Waitall без overlap; ошибка тегов в Sendrecv. Решения: Вставка compute в overlap; явная проверка тегов.

### 7.3. Перспективы улучшения
Добавить non-blocking reductions в CG; тестировать на реальных sparse A; интегрировать с GPU (CUDA-aware MPI).

## 8. Приложения
### 8.1. Исходный код
part1_basic.py
part1_overlap.py
part1_arrays.py
part2_persistent.py
part2_2d.py
part2_compare.py
part3_cg_async.py
analysis.py

### 8.2. Используемые библиотеки и версии
- Python 3.12.3
- mpi4py 3.1.+
- NumPy 1.21.+
- OpenMPI 4.1.+
---
