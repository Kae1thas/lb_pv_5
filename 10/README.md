# ОТЧЕТ
## По лабораторной работе №10: Параллелизация двумерного уравнения теплопроводности

### Сведения о студенте
**Дата:** 2025-11-08
**Семестр:** 1
**Группа:** ПИН-м-о-25-1
**Дисциплина:** Параллельные вычисления
**Студент:** Санамян Олег Арменович

---

## 1. Цель работы
Освоить методы распараллеливания алгоритмов решения многомерных уравнений в частных производных. Реализовать параллельные версии решения двумерного уравнения теплопроводности с использованием одномерной и двумерной декомпозиции расчетной области. Исследовать эффективность различных подходов к распараллеливанию.

## 2. Теоретическая часть
### 2.1. Основные понятия и алгоритмы
Рассматривается начально-краевая задача для двумерного уравнения параболического типа:  
$$ \begin{cases} \varepsilon \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right) - \frac{\partial u}{\partial t} = -u \left( \frac{\partial u}{\partial x} + \frac{\partial u}{\partial y} \right) - u^3, \ (x,y) \in (a,b) \times (c,d), \quad t \in (t_0,T], \\ u(a,y,t) = u_{\text{left}}(y,t), \quad u(b,y,t) = u_{\text{right}}(y,t), \\ u(x,c,t) = u_{\text{bottom}}(x,t), \quad u(x,d,t) = u_{\text{top}}(x,t), \\ u(x,y,t_0) = u_{\text{init}}(x,y). \end{cases} $$  

Для численного решения используется явная схема:  
$$ u_{i,j}^{m+1} = u_{i,j}^m + \tau \left( \varepsilon \left( \frac{u_{i+1,j}^m - 2u_{i,j}^m + u_{i-1,j}^m}{h_x^2} + \frac{u_{i,j+1}^m - 2u_{i,j}^m + u_{i,j-1}^m}{h_y^2} \right) + u_{i,j}^m \left( \frac{u_{i+1,j}^m - u_{i-1,j}^m} {2h_x} + \frac{u_{i,j+1}^m - u_{i,j-1}^m}{2h_y} \right) + (u_{i,j}^m)^3 \right). $$  

Параллелизация достигается путем доменной декомпозиции: одномерной (по оси x) и двумерной (по сетке процессов). Используются призрачные слои для обмена граничными данными между процессами.

### 2.2. Используемые функции MPI
- `MPI.COMM_WORLD`: Получение глобального коммуникатора.
- `MPI.Create_cart`: Создание виртуальной топологии (линейной для 1D, сетчатой для 2D).
- `MPI.Sendrecv`: Обмен данными между соседними процессами (призрачные слои).
- `MPI.Gatherv`: Сбор данных от всех процессов на root (для финального результата).
- `MPI.Barrier`: Синхронизация процессов.
- `MPI.Get_size`, `MPI.Get_rank`: Получение числа процессов и ранга.

## 3. Практическая реализация
### 3.1. Структура программы
Программа состоит из четырех основных модулей:
- `seq_version.py`: Последовательная реализация для базового сравнения (инициализация, цикл вычислений, визуализация).
- `1d_parallel.py`: Параллельная версия с 1D-декомпозицией по оси x (виртуальная топология — линейка процессов).
- `2d_parallel.py`: Параллельная версия с 2D-декомпозицией по осям x и y (сетка процессов, квадратное число процессов).
- `analysis.py`: Анализ результатов (расчет ускорения и эффективности, построение графиков).

Модули взаимодействуют через общие параметры (N_x, N_y, M и т.д.). Параллельные версии используют MPI для распределения нагрузки и обмена данными.

### 3.2. Ключевые особенности реализации
- **Призрачные слои**: Добавление дополнительных слоев для границ поддомена, обеспечивающих корректные вычисления вторых производных.
- **Обмен данными**: Sendrecv для передачи граничных значений между соседями после каждого временного шага.
- **Установка границ**: Константы (0.33) для всех краевых условий, инициализация с функцией u_init (tanh-based).
- **Сбор результатов**: Gatherv на root для 1D; упрощенный локальный вывод для 2D (полный сбор опционален).
- **Визуализация**: Анимация с matplotlib для отображения эволюции поля u.

Решена проблема неравномерного распределения точек с помощью rcounts и displs.

### 3.3. Инструкция по запуску
```bash
# Последовательная версия
python seq_version.py

# 1D-параллельная версия (любое число процессов)
mpiexec -n 4 python 1d_parallel.py

# 2D-параллельная версия (только квадратное число процессов, напр. 4=2x2, 9=3x3, 16=4x4)
mpiexec -n 4 python 2d_parallel.py

# Анализ результатов
python analysis.py
```

## 4. Экспериментальная часть
### 4.1. Тестовые данные
Размер сетки: N_x = 50, N_y = 50 (область [-2, 2] x [-2, 2]).  
Число временных шагов: M = 500.  
Параметр: ε = 10^{-1.5}.  
Граничные условия: константа 0.33.  
Начальное условие: u_init с tanh (круглый "пузырь").

### 4.2. Методика измерений
Измерения времени выполнения с помощью time.time() (от инициализации до сбора).  
Тестирование на числах процессов: 1, 2, 4, 8, 9, 16.  
Условия: Python 3.12+, OpenMPI, локальный кластер (одна машина).  
Каждый тест — один запуск (для reproducibility; в продакшене — усреднение).

### 4.3. Результаты измерений
#### Таблица 1. Время выполнения (секунды)
| Количество процессов | Последовательная | 1D-декомпозиция | 2D-декомпозиция |
|----------------------|------------------|-----------------|-----------------|
| 1                    | 10.9694          | 10.9694         | 10.9694         |
| 2                    | -                | 5.4237          | -               |
| 4                    | -                | 3.7465          | 3.1411          |
| 8                    | -                | 1.8911          | -               |
| 9                    | -                | -               | 1.8307          |
| 16                   | -                | 2.3932          | 2.3406          |

#### Таблица 2. Ускорение (Speedup)
| Количество процессов | 1D-декомпозиция | 2D-декомпозиция |
|----------------------|-----------------|-----------------|
| 1                    | 1.00            | 1.00            |
| 2                    | 2.02            | -               |
| 4                    | 2.93            | 3.49            |
| 8                    | 5.80            | -               |
| 9                    | -               | 5.99            |
| 16                   | 4.58            | 4.69            |

## 5. Визуализация результатов
### 5.1. График времени выполнения
![График времени выполнения](ОТЧЕТ/images/time.png)

### 5.2. График ускорения
![График ускорения](ОТЧЕТ/images/speedup.png)

### 5.3. График эффективности
![График эффективности](ОТЧЕТ/images/efficiency.png)

## 6. Анализ результатов
### 6.1. Анализ производительности
Ускорение растет с числом процессов, но не линейно: для 1D — до 5.8 при 8 процессах, для 2D — до 5.99 при 9. Эффективность падает (с 1.0 до ~0.29 для 16), что соответствует закону Амдала из-за коммуникаций. 2D-декомпозиция эффективнее для больших P (меньше обменов на процесс).

### 6.2. Сравнение с теоретическими оценками
Теоретическое идеальное ускорение — P, но реальное ниже из-за overhead (обмен ~O(P) сообщений). Для 4 процессов: теория 4, реальность 2.93 (1D) и 3.49 (2D) — близко, с учетом коммуникаций.

### 6.3. Выявление узких мест
Коммуникации (Sendrecv) доминируют при малых поддоменах. Дисбаланс нагрузки минимален (равномерное деление). Для 2D: ограничение на квадрат P; для 16 в 1D — рост времени из-за overhead.

## 7. Заключение
### 7.1. Выводы
Реализованы последовательная, 1D- и 2D-параллельные версии. Достигнуто ускорение до 6x. 2D-декомпозиция предпочтительна для квадратных сеток. Визуализация подтверждает корректность (эволюция "пузыря").

### 7.2. Проблемы и решения
Проблема: не-квадратные P для 2D — решение: проверка и abort. Overhead обмена — минимизирован Sendrecv. Сбор данных в 2D упрощен (локальный вывод).

### 7.3. Перспективы улучшения
Оптимизация: неявная схема для стабильности, Allgatherv для полного 2D-сбора, GPU-ускорение (CuPy). Тестирование на кластере для большего P.

## 8. Приложения
### 8.1. Исходный код
- `seq_version.py`: Последовательная реализация.
- `1d_parallel.py`: 1D-параллельная версия.
- `2d_parallel.py`: 2D-параллельная версия.
- `analysis.py`: Анализ и графики.

Полный код доступен в репозитории.

### 8.2. Используемые библиотеки и версии
- Python 3.12+
- mpi4py 3.1.+
- NumPy 1.21.+
- Matplotlib 3.5.+
- OpenMPI 4.1.+

---
